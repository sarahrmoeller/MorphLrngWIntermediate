{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import random\n",
    "\n",
    "def extractlexc(Replines, Flags=True, Boundaries=True):\n",
    "    '''Takes list of foma output with transitions.\n",
    "     Puts input and output lines in dict. Keeps flags'''\n",
    "    \n",
    "    MORPH_FLAGS = set(['@U.IC.Yes@','@U.StemInitial.Yes@','@U.StemFinal.Yes@','@U.PERNUM.3SGSAI@','@U.PERNUM.3PLSAI@',\n",
    "                   '@U.PERNUM.4SGSAI@','@U.PERNUM.4PLSAI@','@U.PERNUM.3SGTI@','@U.PERNUM.3PLTI@','@U.PERNUM.4SGTI@',\n",
    "                   '@U.PERNUM.4PLTI@','@U.PERNUM.3S4S@','@U.PERNUM.3S4PL@','@U.PERNUM.4S4S@','@U.PERNUM.4S4PL@',\n",
    "                   '@U.PERNUM.3PL4S@','@U.PERNUM.3PL4PL@','@U.PERNUM.4PL4S@','@U.PERNUM.4PL4PL@','@U.PERNUM.4S3S@',\n",
    "                   '@U.PERNUM.4PL3S@','@U.PERNUM.4S4S@','@U.PERNUM.4S4PL@','@U.PERNUM.4S3PL@','@U.PERNUM.4PL3PL@',\n",
    "                   '@U.PERNUM.4S4PL@','@U.PERNUM.4PL4PL@','@U.Polarity.Negative@','@U.TENSE.Past@','@U.Mood.Question@',\n",
    "                   '@U.Order.NAFF@','@U.StemType.II@','@U.StemType.AI@'])\n",
    "\n",
    "    translib = {}\n",
    "    \n",
    "    for line in Replines:\n",
    "        inputline = []\n",
    "        outputline = []\n",
    "        for element in line:\n",
    "            inp = ''\n",
    "            outp = ''\n",
    "            #split if transition\n",
    "            if ':' in element:\n",
    "                inp,outp = element.split(':')\n",
    "            #if no transition, element is both input and output\n",
    "            else:\n",
    "                inp = element\n",
    "                outp = element\n",
    "            #leave out null elements \n",
    "            if inp != '0':\n",
    "                #filter flags in UR not affecting phonology\n",
    "                if inp.startswith('@'):\n",
    "                    #print(inp)\n",
    "                    if Flags and inp in MORPH_FLAGS:\n",
    "                            inputline.append(inp)\n",
    "                else:\n",
    "                    inputline.append(inp)\n",
    "            if outp != '0':\n",
    "                #filter flags in UR not affecting phonology\n",
    "                if outp.startswith('@'):\n",
    "                    #print(inp)\n",
    "                    if Flags and outp in MORPH_FLAGS:\n",
    "                        outputline.append(outp)\n",
    "                elif outp == '^':\n",
    "                    if Boundaries:\n",
    "                        outputline.append(outp)\n",
    "                else:\n",
    "                    outputline.append(outp)\n",
    "        inputstring = ' '.join(inputline)\n",
    "        outputstring = ' '.join(outputline)\n",
    "        #add input:output to dict, allow for multiple IR/SR representations\n",
    "        if inputstring not in translib:\n",
    "            translib[inputstring] = [outputstring]\n",
    "        else:\n",
    "            translib[inputstring].append(outputstring)\n",
    "    \n",
    "    return translib\n",
    "\n",
    "def extractfoma(Replines, Flags=True):\n",
    "    '''Takes list of foma output with transitions.\n",
    "     Puts input and output lines in dict. Keeps flags'''\n",
    "    \n",
    "    MORPH_FLAGS = set(['@U.IC.Yes@','@U.StemInitial.Yes@','@U.StemFinal.Yes@','@U.PERNUM.3SGSAI@','@U.PERNUM.3PLSAI@',\n",
    "                   '@U.PERNUM.4SGSAI@','@U.PERNUM.4PLSAI@','@U.PERNUM.3SGTI@','@U.PERNUM.3PLTI@','@U.PERNUM.4SGTI@',\n",
    "                   '@U.PERNUM.4PLTI@','@U.PERNUM.3S4S@','@U.PERNUM.3S4PL@','@U.PERNUM.4S4S@','@U.PERNUM.4S4PL@',\n",
    "                   '@U.PERNUM.3PL4S@','@U.PERNUM.3PL4PL@','@U.PERNUM.4PL4S@','@U.PERNUM.4PL4PL@','@U.PERNUM.4S3S@',\n",
    "                   '@U.PERNUM.4PL3S@','@U.PERNUM.4S4S@','@U.PERNUM.4S4PL@','@U.PERNUM.4S3PL@','@U.PERNUM.4PL3PL@',\n",
    "                   '@U.PERNUM.4S4PL@','@U.PERNUM.4PL4PL@','@U.Polarity.Negative@','@U.TENSE.Past@','@U.Mood.Question@',\n",
    "                   '@U.Order.NAFF@','@U.StemType.II@','@U.StemType.AI@'])\n",
    "    translib = {}\n",
    "    \n",
    "    for line in Replines:\n",
    "        inputline = []\n",
    "        outputline = []\n",
    "        for element in line:\n",
    "            inp = ''\n",
    "            outp = ''\n",
    "            #split if transition\n",
    "            if ':' in element:\n",
    "                inp,outp = element.split(':')\n",
    "            #if no transition, element is both input and output\n",
    "            else:\n",
    "                inp = element\n",
    "                outp = element\n",
    "            #leave null elements out \n",
    "            if inp != '0':\n",
    "                #filter flags in UR not affecting phonology\n",
    "                if inp.startswith('@') and Flags:\n",
    "                    if inp in MORPH_FLAGS:\n",
    "                        inputline.append(inp)\n",
    "                else:\n",
    "                    inputline.append(inp)\n",
    "            if outp != '0':\n",
    "                #filter out boundaries and flags from SR\n",
    "                if not outp.startswith('@') and outp != '^':\n",
    "                    outputline.append(outp)\n",
    "        inputstring = ' '.join(inputline)\n",
    "        outputstring = ' '.join(outputline)\n",
    "        #add input:output to dict, allow for multiple IR/SR representations\n",
    "        if inputstring not in translib:\n",
    "            translib[inputstring] = [outputstring]\n",
    "        else:\n",
    "            translib[inputstring].append(outputstring)\n",
    "    \n",
    "    return translib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def combineLibs(lib1, lib2):\n",
    "    '''Takes URIR and URSR dicts \n",
    "    Returns dic with IR and SR lists as value for the corresponding UR'''\n",
    "    \n",
    "    lib = {}\n",
    "    ct = 0\n",
    "    for UR,Rep1 in lib1.items():\n",
    "        lib[UR] = [Rep1]\n",
    "        try:\n",
    "            lib[UR].append(lib2[UR])\n",
    "        except KeyError:\n",
    "            ct +=1\n",
    "            #print('Not found in second dict', ''.join(UR))\n",
    "    print(ct)\n",
    "    return lib\n",
    "\n",
    "def unambiguate(combineddict):\n",
    "    '''Takes {UR:[[IR],[SR]]} and limits to those with 1-1-1 mapping\n",
    "    Returns three lists.'''\n",
    "    \n",
    "    URlist = []\n",
    "    IRlist = []\n",
    "    SRlist = []\n",
    "    dbmistakes = ('a','l')\n",
    "    \n",
    "    for UR,reps in combineddict.items():\n",
    "        if len(reps) == 2:\n",
    "            if len(reps[0]) == 1 and len(reps[1]) == 1:\n",
    "                if dbmistakes[0] in reps[0] or dbmistakes[1] in reps[0] or dbmistakes[0] in reps[1] or dbmistakes[1] in reps[1]:\n",
    "                    break\n",
    "                else:\n",
    "                    URlist.append(UR)\n",
    "                    IRlist.append(reps[0])\n",
    "                    SRlist.append(reps[1])\n",
    "    \n",
    "    return URlist,IRlist,SRlist\n",
    "\n",
    "    \n",
    "def unambiguous2File(URlist,IRlist,SRlist,genRparse):\n",
    "    '''Takes UR,IR,SR list, splits data 8/1/1, \n",
    "    prints to files, each line corresponding\n",
    "    Not used.'''\n",
    "    \n",
    "    filenames = ['URtrain','URdev','URgold','IRtrain','IRdev','IRgold','SRtrain','SRdev','SRgold']\n",
    "    \n",
    "    #split data to 80/10/10\n",
    "    URtrain,URsplit,IRtrain,IRsplit,SRtrain,SRsplit = train_test_split(URlist, IRlist, SRlist, test_size=.2)\n",
    "    URdev,URtest,IRdev,IRtest,SRdev,SRtest = train_test_split(URsplit,IRsplit,SRsplit, test_size=.5)\n",
    "    \n",
    "    datasets = [URtrain,URdev,URtest,IRtrain,IRdev,IRtest,SRtrain,SRdev,SRtest]\n",
    "    \n",
    "    for idx,name in enumerate(filenames):\n",
    "        cur_file = genRparse + '-' + name + '.txt'\n",
    "        with open(cur_file, 'w') as F:\n",
    "            if name.startswith('UR'):\n",
    "                F.write(''.join(datasets[idx]))\n",
    "            else:\n",
    "                stringrep = [''.join(element) for element in datasets[idx]]\n",
    "                F.write(''.join(stringrep))\n",
    "                \n",
    "\n",
    "def formatInputOutput(inputlist,outputlist,thirdlist,filename):\n",
    "    '''Takes third lists from unambiguate method.\n",
    "    Formats for processdump method.'''\n",
    "    \n",
    "    masterlist = []\n",
    "    \n",
    "    for idx,line in enumerate(inputlist):\n",
    "        masterlist.append(line)\n",
    "        masterlist.append(''.join(outputlist[idx]))\n",
    "        masterlist.append(''.join(thirdlist[idx]))\n",
    "    with open(filename, 'w') as M:\n",
    "        masterstring = ''.join(masterlist)\n",
    "        M.write(masterstring)\n",
    "    return len(masterlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def writeOpenNMTfiles(data,rep,seed):\n",
    "    '''Helper to process dump\n",
    "    Takes datalist. Splits 40/40/10/10.\n",
    "    Writes train1, test&train, dev, and test2.'''\n",
    "    export = []\n",
    "    #3 way split\n",
    "    split1,test1train2 = train_test_split(data,test_size=0.55, random_state=seed)\n",
    "    train1,val1 = train_test_split(split1,test_size=0.2, random_state=seed)\n",
    "    \n",
    "    splits = ['-val1.txt','-test1train2.txt', '-train1.txt']\n",
    "    for split in splits:\n",
    "        if split == '-test1train2.txt':\n",
    "            export = test1train2\n",
    "        elif split == '-train1.txt':\n",
    "            export = train1\n",
    "        else:\n",
    "            export = val1\n",
    "        #write to file\n",
    "        file = rep + split\n",
    "        with open(file, 'w') as F:\n",
    "            for line in export:\n",
    "                F.write(line)\n",
    "                \n",
    "def reduceTraining(q,num_ex):\n",
    "    '''Returns q random indices from total number of data lines.'''\n",
    "    idcs = []\n",
    "    for i in range(q):\n",
    "        idcs.append(random.randint(0,num_ex))\n",
    "    return idcs\n",
    "                \n",
    "def processdump(masterfile,representation,quantity=5000):\n",
    "    '''Takes file from formatInputOutput method where lines in order: UR,IR,SR.\n",
    "    Prepares different files for OpenNMT with randomly chosen repeated lines.'''\n",
    "    \n",
    "    datalines = [line for line in open(masterfile)]\n",
    "    \n",
    "    inputline = []\n",
    "    outputline = []\n",
    "    thirdline = []\n",
    "    inputline2 = []\n",
    "    outputline2 = []\n",
    "    thirdline2 = []\n",
    "    #mistakes carried over from Arapaho database\n",
    "    #dbmistakes = ['?','E','I','N','K','O','a','l']\n",
    "\n",
    "    for linum,line in enumerate(datalines):\n",
    "        #process UR, IR, and SR in same code block (so can skip all if needed)\n",
    "        if linum % 3 == 0:\n",
    "            #get gold lines\n",
    "            inputline.append(line)\n",
    "            outputline.append(''.join(datalines[linum+1]))\n",
    "            thirdline.append(''.join(datalines[linum+2]))\n",
    "            #Repeat lines for oNMT training\n",
    "            inp2 = line[:-1] + line\n",
    "            outp = ''.join(datalines[linum+1])\n",
    "            outp2 = outp[:-1] + outp\n",
    "            third = ''.join(datalines[linum+2])\n",
    "            third2 = third[:-1] + third\n",
    "            inputline2.append(''.join(inp2))\n",
    "            outputline2.append(''.join(outp2))\n",
    "            thirdline2.append(''.join(third2))\n",
    "    \n",
    "    #Same random split for surf & tags\n",
    "    seed = random.randint(1,100)\n",
    "    #To randomize and reduce data\n",
    "    if quantity < len(datalines):\n",
    "        rand_indices = reduceTraining(quantity,len(inputline))\n",
    "        inputline = [inputline[idx] for idx in rand_indices]\n",
    "        outputline = [outputline[idx] for idx in rand_indices]\n",
    "        thirdline = [thirdline[idx] for idx in rand_indices]\n",
    "        inputline2 = [inputline2[idx] for idx in rand_indices]\n",
    "        outputline2 = [outputline2[idx] for idx in rand_indices]\n",
    "        thirdline2 = [thirdline2[idx] for idx in rand_indices]\n",
    "    #splits data and writes files\n",
    "    writeOpenNMTfiles(inputline,representation[0],seed)\n",
    "    writeOpenNMTfiles(outputline,representation[1],seed)\n",
    "    writeOpenNMTfiles(thirdline,representation[2],seed)\n",
    "    writeOpenNMTfiles(inputline2,representation[3],seed)\n",
    "    writeOpenNMTfiles(outputline2,representation[4],seed)\n",
    "    writeOpenNMTfiles(thirdline2,representation[5],seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexcfile = os.path.join(r'C:\\Users\\Sarah R M\\Documents\\CU\\Arapaho\\ComputELppr', 'lexcWordsFlagsCorrected3')\n",
    "URIRlines = [line.split(' ') for line in open(lexcfile)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fwBwFfile = os.path.join(r'C:\\Users\\Sarah R M\\Documents\\CU\\Arapaho\\ComputELppr', 'fomaNoBoundaryNoFlagWithSpaceCorrected3')\n",
    "URSRlines = [line.split(' ') for line in open(fwBwFfile)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "URIRlib = extractlexc(URIRlines,Flags=False,Boundaries=False)\n",
    "#URIRnFlagslib = extractRep(URIRlines, generation=False)\n",
    "URSRlib = extractfoma(URSRlines, Flags=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#unambiguous2File(unambiguate(combineLibs(URIRlib,URSRlib)), 'parse')\n",
    "\n",
    "comboURIRSR = combineLibs(URIRlib,URSRlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "UR,IR,SR = unambiguate(comboURIRSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#unambiguous2File(UR,IR,SR, 'gen')\n",
    "fulllines = formatInputOutput(UR,IR,SR,'MasterFileAL2nFnB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#processdump('MasterFileALnFnB',['URnFnB125kf0','IRnFnB125kf0','SRnFnB125f0','URnFnB125k2f0','IRnFnB125k2f0','SRnFnB125k2f0'],quantity=125000)\n",
    "processdump('MasterFileALnFnB',['URnFnB100kf0','IRnFnB100kf0','SRnFnB100kf0','URnFnB100k2f0','IRnFnB100k2f0','SRnFnB100k2f0'],quantity=100000)\n",
    "#processdump('MasterFileALnFnB',['URnFnB75k','IRnFnB75k','SRnFnB75k','URnFnB75k2','IRnFnB75k2','SRnFnB75k2'],quantity=75000)\n",
    "#processdump('MasterFilenFnB',['URnFnB12k','IRnFnB12k','SRnFnB12k','URnFnB12k2','IRnFnB12k2','SRnFnB12k2'],quantity=12000)\n",
    "#processdump('MasterFileALnFnB',['URnFnB87k','IRnFnB87k','SRnFnB87k','URnFnB87k2','IRnFnB87k2','SRnFnB87k2'],quantity=87500)\n",
    "#processdump('MasterFileALnFnB',['JURnFnB50k','JIRnFnB50k','JSRnFnB50k','JURnFnB50k2','JIRnFnB50k2','JSRnFnB50k2'],quantity=50000)\n",
    "#processdump('MasterFilenFnB',['URnFnB','IRnFnB','SRnFnB','URnFnB2','IRnFnB2','SRnFnB2'],quantity=len(open('MasterFilenFnB').readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "\n",
    "def distance(str1, str2):\n",
    "    \"\"\"Simple Levenshtein implementation for evalOpenNMT.\"\"\"\n",
    "    m = np.zeros([len(str2)+1, len(str1)+1])\n",
    "    for x in range(1, len(str2) + 1):\n",
    "        m[x][0] = m[x-1][0] + 1\n",
    "    for y in range(1, len(str1) + 1):\n",
    "        m[0][y] = m[0][y-1] + 1\n",
    "    for x in range(1, len(str2) + 1):\n",
    "        for y in range(1, len(str1) + 1):\n",
    "            if str1[y-1] == str2[x-1]:\n",
    "                dg = 0\n",
    "            else:\n",
    "                dg = 1\n",
    "            m[x][y] = min(m[x-1][y] + 1, m[x][y-1] + 1, m[x-1][y-1] + dg)\n",
    "    return int(m[len(str2)][len(str1)])\n",
    "\n",
    "\n",
    "def delEndTags(predfile,goldfile):\n",
    "    '''Takes results from repeated data & gold.\n",
    "    Deletes all elements not corresponding to an element in gold.\n",
    "    Writes to new file for evaluation.'''\n",
    "    testlines = [line.strip().split(' ') for line in open(goldfile)]\n",
    "    predlines = [line.strip().split(' ') for line in open(predfile)]\n",
    "    \n",
    "    reducedlines = []\n",
    "    for linidx,predline in enumerate(predlines):\n",
    "        newline = []\n",
    "        if len(predline) > len(testlines[linidx]):\n",
    "            newline = predline[:len(testlines[linidx])]\n",
    "        else:\n",
    "            newline = predline\n",
    "        reducedlines.append(newline)\n",
    "    return reducedlines, testlines\n",
    "\n",
    "def evalOpenNMT(predfile,goldfile,note,file='Report-',interm=False):\n",
    "    '''Takes pred eval file from OpenNMT and test file. \n",
    "    Compares. Returns PR and F1. \n",
    "    Prints report to file.'''\n",
    "    \n",
    "    evallines, goldlines = delEndTags(predfile,goldfile)\n",
    "    \n",
    "    TPFNlines = len(goldlines)\n",
    "    TPFPlines = len(evallines)\n",
    "    lines_correct = 0\n",
    "    correct_tagcts = {}\n",
    "    incorrect_tagcts = {}\n",
    "    gold_tagscts = {}\n",
    "    reportstring = ''\n",
    "    dist = 0.\n",
    "    \n",
    "    #get total gold tags\n",
    "    for line in goldlines:\n",
    "        for gold_element in line:\n",
    "            gold_tagscts[gold_element] = gold_tagscts.get(gold_element, 0) + 1\n",
    "    \n",
    "    #tag/element accuracy\n",
    "    for linidx, line in enumerate(evallines):\n",
    "        #line counts\n",
    "        if line == goldlines[linidx]:\n",
    "            lines_correct += 1\n",
    "        else:\n",
    "            predstr = ''.join(line)\n",
    "            goldstr = ''.join(goldlines[linidx])\n",
    "            with open('diffPred-' + predfile, 'a') as D:\n",
    "                D.write(\"Pred: \" + predstr + '\\n' + \n",
    "                        'Gold: ' + goldstr +'\\n')\n",
    "            dist += distance(goldstr, predstr)\n",
    "\n",
    "        #allow for different line lengths\n",
    "        shorter = evallines[linidx]\n",
    "        longer = goldlines[linidx]\n",
    "        if len(evallines[linidx]) > len(goldlines[linidx]):\n",
    "            shorter = goldlines[linidx]\n",
    "            longer = evallines[linidx]\n",
    "            for tag in evallines[linidx][(len(shorter) - len(longer)):]:\n",
    "                incorrect_tagcts[tag] = incorrect_tagcts.get(tag,0) + 1\n",
    "        #count tags\n",
    "        for tagidx,tag in enumerate(shorter):\n",
    "            try:\n",
    "                if tag == longer[tagidx]:\n",
    "                    correct_tagcts[tag] = correct_tagcts.get(tag, 0) + 1\n",
    "                else:\n",
    "                    incorrect_tagcts[evallines[linidx][tagidx]] = incorrect_tagcts.get(evallines[linidx][tagidx], 0) + 1\n",
    "            except IndexError:\n",
    "                print(tagidx, len(longer), longer[tagidx], len(shorter), tag)\n",
    "    \n",
    "    #overall accuracy\n",
    "    try:\n",
    "        linesP = lines_correct/TPFPlines\n",
    "        linesR = lines_correct/TPFNlines\n",
    "        avgdist = round(dist/TPFNlines, 2)\n",
    "        reportstring += \"%s\\n%s\\n\\n Num lines: %d\\n All lines Prec: %.4f\\n All lines Recall: %.4f\\n All lines F1: %.4f\\n\\n levenshtein:\\t%.2f\\n\" % (predfile,note,TPFNlines,round(linesP,5),round(linesR,5),round((2*linesP*linesR)/(linesP+linesR),15),avgdist)\n",
    "    except ZeroDivisionError:\n",
    "        reportstring += '%s\\n%s\\n\\n Num lines: %d\\n All lines Accuracy: %.4f\\n\\n levenshtein:\\t%.2f\\n' %(predfile,note,TPFNlines,round(lines_correct/TPFPlines),avgdist)\n",
    "    \n",
    "    #all tags in file, PR&F1        \n",
    "    num_allcorrect = sum(correct_tagcts.values())\n",
    "    num_allincorrect = sum(incorrect_tagcts.values())\n",
    "    allPrec = num_allcorrect/(num_allcorrect+num_allincorrect)\n",
    "    allRecall = num_allcorrect/sum(gold_tagscts.values())\n",
    "    reportstring += \"\\n  All tags Precision: %.4f\\n  All tags Recall: %.4f\\n  All tags F1: %.4f\\n\" % (round(allPrec,5),round(allRecall,5),round((2*allPrec*allRecall)/(allPrec+allRecall),10))\n",
    "    \n",
    "    #individual tags, PR&F1\n",
    "    reportstring += \"\\n              TAG   \\t PREC\\t RECALL\\t  F1\\tinstances \\n\"\n",
    "    tag_precisions = {}\n",
    "    tag_recalls = {}\n",
    "    tag_F1s = {}\n",
    "    avgP = 0.00\n",
    "    avgR = 0.00\n",
    "    avgF1 = 0.00\n",
    "    for tag in sorted(gold_tagscts.keys()):\n",
    "        try:\n",
    "            tag_precisions[tag] = correct_tagcts.get(tag, 0.00)/(correct_tagcts.get(tag,0.00) +\n",
    "                                                                 incorrect_tagcts.get(tag,0.00))\n",
    "        except ZeroDivisionError:\n",
    "            tag_precisions[tag] = 0.00\n",
    "            instances = 0\n",
    "        try:\n",
    "            tag_recalls[tag] = correct_tagcts[tag]/gold_tagscts[tag]\n",
    "        except (ZeroDivisionError,KeyError):\n",
    "            tag_recalls[tag] = 0.00\n",
    "        try:\n",
    "            tag_F1s[tag] = (2 * tag_precisions[tag] * tag_recalls[tag])/(tag_precisions[tag]\n",
    "                                                                          + tag_recalls[tag])\n",
    "        except ZeroDivisionError:\n",
    "            tag_F1s[tag] = 0.00\n",
    "        avgP += tag_precisions[tag]\n",
    "        avgR += tag_recalls[tag]\n",
    "        avgF1 += tag_F1s[tag]\n",
    "        uniq_tags = len(gold_tagscts.keys())\n",
    "        reportstring += \"  %20s\\t %.4f\\t %.4f\\t %.4f\\t %d\\n\" % (tag,\n",
    "                                                           tag_precisions[tag],\n",
    "                                                           tag_recalls[tag],\n",
    "                                                           tag_F1s[tag],\n",
    "                                                           gold_tagscts[tag])\n",
    "    reportstring += \"    total/average: \\t %.4f\\t %.4f\\t %.4f\\t %d\" %(round((avgP/uniq_tags),5),\n",
    "                                                                     round((avgR/uniq_tags),5),\n",
    "                                                                     round((avgF1/uniq_tags),5),\n",
    "                                                                    sum(gold_tagscts.values()))\n",
    "                                                                    \n",
    "    print(num_allcorrect, num_allincorrect, sum(gold_tagscts.values()))\n",
    "    with open(file+predfile, \"w\") as R:     \n",
    "        R.write(reportstring)\n",
    "    \n",
    "    if interm:\n",
    "        return evallines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1088154 6668 1094822\n"
     ]
    }
   ],
   "source": [
    "#evalOpenNMT('SR-URnFnB25k-eval.txt','URnFnB25k-test2.txt','\\n 40/10/10 split. No flags or boundaries.\\nRepeated lines in training and test, deleted for evaluation.\\n25 epochs 1.05 perplexity',file='Report-')\n",
    "#evalOpenNMT('SR-IRnFnB25k-eval1.txt','IRnFnB25k-test1train2.txt','\\n 40/10/40 split. No flags or boundaries.\\nRepeated lines in training and test, deleted for evaluation.\\n25 epochs 1.15 perplexity',file='Report-')\n",
    "#evalOpenNMT('SR-IR-URnFnB25k-eval.txt','URnFnB25k-test2.txt','\\n 40/10/10 split. No flags or boundaries.\\nRepeated lines in training and test, deleted for evaluation.\\n25 epochs ?',file='Report-')\n",
    "IRALreduced = evalOpenNMT('UR-SRnFnB125k2f0-output.txt','SRnFnB125f0-test1train2.txt','\\n 45K training lines  36/9/55 split. NO flags or boundaries.\\nRepeated lines in training and test, deleted for evaluation.\\n20 epochs, 1.00 PPL',file='Report-',interm=True)\n",
    "#evalOpenNMT('IR-SRnFnB125k2f0-output2.txt','SRnFnB100k2f0-test2.txt','\\n 36301 training lines. 66/12/12 split.\\nNO flags or boundaries.\\nRepeated lines in training and test, deleted for evaluation.\\n20 epochs. 1.?? perplexity', file='Report-')\n",
    "#evalOpenNMT('SR-URnFnB125k2-pred.txt','URnFnB125k-test2.txt','\\n 45375 training lines. 66/12/12 split.\\nNO flags or boundaries.\\nRepeated lines in training and test, deleted for evaluation.\\n20 epochs',file='Report-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def repeatLines(predtesttrain):\n",
    "    '''Takes eval file for intermediate prediction.\n",
    "    Repeat lines for training second LSTM in OpenNMT.'''\n",
    "    \n",
    "    return [' '.join(line)+' '+ ' '.join(line) +'\\n' for line in predtesttrain]\n",
    "    \n",
    "        \n",
    "def newsplit(seeddata,rep):\n",
    "    '''Takes unrepeated lines as list from itermediate predictions and repeats lines. \n",
    "    Or takes 2-test1train2 data file. \n",
    "    Returns train,dev,test files for OpenNMT.'''\n",
    "    \n",
    "    data = []\n",
    "    if isinstance(seeddata, list):\n",
    "        data = repeatLines(seeddata)\n",
    "    else:\n",
    "        data = [line for line in open(seeddata)]\n",
    "        \n",
    "    export = []\n",
    "    #3 way split\n",
    "    split1,train2 = train_test_split(data,test_size=0.66, random_state=45)\n",
    "    test2,val2 = train_test_split(split1,test_size=0.5, random_state=45)\n",
    "    \n",
    "    splits = ['-val2.txt','-train2.txt', '-test2.txt']\n",
    "    for split in splits:\n",
    "        if split == '-train2.txt':\n",
    "            export = train2\n",
    "        elif split == '-test2.txt':\n",
    "            export = test2\n",
    "        else:\n",
    "            export = val2\n",
    "        #write to file\n",
    "        file = rep + split\n",
    "        with open(file, 'w') as F:\n",
    "            for line in export:\n",
    "                F.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsplit(IRALreduced, 'IRnFnB100k2f0')\n",
    "newsplit('SRnFnB100k2f0-test1train2.txt','SRnFnB100k2f0')\n",
    "newsplit('URnFnB100k2f0-test1train2.txt','URnFnB100k2f0')\n",
    "newsplit('URnFnB100k2f0-test1train2.txt','URnFnB100k2f0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(open('IRnFnB125k2f0-train1.txt').readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
